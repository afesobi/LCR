import nbformat

# Load the existing notebook
notebook_path = "/mnt/data/LRC_ASSIGNMENT_1.ipynb"
with open(notebook_path, "r", encoding="utf-8") as f:
    notebook = nbformat.read(f, as_version=4)

# Add the provided Python code with print statements to the notebook
new_cells = [
    nbformat.v4.new_code_cell("""
import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import recall_score, precision_score
from sklearn.model_selection import cross_validate
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

from sklearn.datasets import load_wine

# Load the Wine dataset
wine_data = load_wine()

# Convert to DataFrame
wine_df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)

# Bind the 'class' (wine target) to the DataFrame
wine_df['class'] = wine_data.target

# Display the DataFrame
print(wine_df.head())
"""),
    nbformat.v4.new_code_cell("""
# Question 1

# (i) Number of observations
n_observations = wine_df.shape[0]
print("Number of observations:", n_observations)

# (ii) Number of variables
n_variables = wine_df.shape[1]
print("Number of variables:", n_variables)

# (iii) Data type and unique classes
type_class = wine_df['class'].dtype
unique_classes = wine_df['class'].unique()
print("Data type of 'class':", type_class)
print("Unique classes:", unique_classes)

# (iv) Number of predictor variables
n_predictors = wine_df.shape[1] - 1
print("Number of predictor variables:", n_predictors)
"""),
    nbformat.v4.new_code_cell("""
# Question 2

# Select predictors (excluding the last column)
predictors = wine_df.iloc[:, :-1]

# Standardize the predictors
scaler = StandardScaler()
predictors_standardized = pd.DataFrame(scaler.fit_transform(predictors), columns=predictors.columns)

# Display the head of the standardized predictors
print("Standardized predictors:")
print(predictors_standardized.head())

# (iii) Set a random seed
random.seed(123)
print("Random seed set to 123")

# (iv) Split the data into training (75%) and testing (25%) sets
X_train, X_test, y_train, y_test = train_test_split(
    predictors_standardized, wine_df['class'], test_size=0.25, random_state=123
)

# Display the shapes of the training and testing sets to confirm the split
print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)
"""),
    nbformat.v4.new_code_cell("""
# Question 3: Model Initialization and Cross-Validation

# 1. Initialize the KNN classifier
knn = KNeighborsClassifier()

# 2. Define parameter grid for n_neighbors (1 to 50)
param_grid = {'n_neighbors': list(range(1, 51))}

# 3. Implement grid search with 10-fold cross-validation
grid_search = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# 4. Identify and return the best value for n_neighbors
best_n_neighbors = grid_search.best_params_['n_neighbors']
print("Best n_neighbors:", best_n_neighbors)
"""),
    nbformat.v4.new_code_cell("""
# Question 4: Model Evaluation

# 1. Fit the KNN model with the best hyperparameter
best_knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)
best_knn.fit(X_train, y_train)

# 2. Make predictions on the test set
y_pred = best_knn.predict(X_test)

# 3. Evaluate the model using accuracy_score
accuracy = accuracy_score(y_test, y_pred)

# Print the accuracy score
print("Test Set Accuracy:", accuracy)
""")
]

# Append new cells to the notebook
notebook["cells"].extend(new_cells)

# Save the modified notebook
updated_notebook_path = "/mnt/data/LRC_ASSIGNMENT_1_UPDATED.ipynb"
with open(updated_notebook_path, "w", encoding="utf-8") as f:
    nbformat.write(notebook, f)

# Provide the updated notebook file
updated_notebook_path
